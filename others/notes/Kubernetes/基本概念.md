# K8s基本概念

Kubernetes中的大部分概念如Node、Pod、Replication Controller、Service等都可以被看作一种资源对象，几乎所有资源对象都可以通过Kubernetes提供的kubectl工具（或者API编程调用）执行增、删、改、查等操作并将其保存在etcd中持久化存储。

相关概念主要包括：

Master, Node, Pod, Label, Replication Controller, Deployment, Horizontal Pod Autoscaler, StatefulSet, Service, Job, Volume,  Persistent Volume, Namespace, Annotation, ConfigMap

<br>

## Master

Kubernetes里的Master指的是集群控制节点，在每个K8s集群里都需要有一个Master来负责整个集群的管理和控制，基本上K8s的所有控制命令都发给它，它负责具体的执行过程。

Master通常会占据一个独立的服务器（高可用部署建议用3台服务器），如果它宕机或者不可用，那么对集群内容器应用的管理都将失效。

在Master上运行着以下关键进程：

- Kubernetes API Server（kube-apiserver）：提供了HTTP Rest接口的关键服务进程，是Kubernetes里所有资源的增、删、改、查等操作的唯一入口，也是集群控制的入口进程）
- Kubernetes Controller Manager（kube-controller-manager）：所有资源对象的自动化控制中心
- Kubernetes Scheduler（kube-scheduler）：负责资源调度的进程。

## Node

除了Master，Kubernetes集群中的其他机器被称为Node.

与Master一样，Node可以是一台物理主机，也可以是一台虚拟机。

Node是K8s集群中的工作负载节点，每个Node都会被Master分配一些工作负载（Docker容器），当某个Node宕机时，其上的工作负载会被Master自动转移到其他节点上。

在每个Node上都运行着以下关键进程：

- kubelet：负责Pod对应的容器的创建、启停等任务，同时与Master密切协作，实现集群管理的基本功能。
- kube-proxy：实现Kubernetes Service的通信与负载均衡机制的重要组件。
- Docker Engine（docker）：Docker引擎，负责本机的容器创建和管理工作

Node可以在运行期间动态增加到Kubernetes集群中，前提是在这个节点上已经正确安装、配置和启动了上述关键进程。在默认情况下kubelet会向Master注册自己。

一旦Node被纳入集群管理范围，kubelet进程就会定时向Master汇报自身的信息。Master根据每个Node的资源使用情况，并实现高效均衡的资源调度策略。超过指定时间不上报信息时，会被Master判定为“失联”，Node的状态被标记为不可用（Not Ready）。

## Pod

Pod是Kubernetes最重要的基本概念，是可以在 Kubernetes 中创建和管理的、最小的可部署的计算单元。

在组成上，每个Pod都有一个特殊的被称为“根容器”的Pause容器。此外，每个Pod还包含一个或多个紧密相关的用户业务容器。Pause容器与业务无关，其状态可以代表整个容器组的状态，用于判断整体是否死亡。其次，共享Pause容器挂接的Volume，可以简化业务容器之间的通信问题，并解决业务容器之间的文件共享问题。

Kubernetes为每个Pod都分配了唯一的IP地址，称之为Pod IP，一个Pod里的多个容器共享Pod IP地址。

Pod有两种类型：普通的Pod及静态Pod（Static Pod）。后者比较特殊，它并没被存放在Kubernetes的etcd存储里，而是被存放在某个具体的Node上的一个具体文件中，并且只在此Node上启动、运行。普通的Pod一旦被创建，就会被放入etcd中存储，随后会被Master调度到某个具体的Node上并进行绑定（Binding），随后该Pod被
对应的Node上的kubelet进程实例化成一组相关的Docker容器并启动。

默认情况下，当Pod里的某个容器停止时，Kubernetes会自动检测到这个问题并且重新启动这个Pod（重启Pod里的所有容器），如果Pod所在的Node宕机，就会将这个Node上的所有Pod重新调度到其他节点上

> Pod、容器与Node的关系，简单来说就是有好多物理主机，其中一台（或者更多台）作为Master并负责调度，其他的是Node，Node里有若干Pod，每个Pod里装了一组业务相关的容器（例如docker容器）。

Docker Volume在Kubernetes里也有对应的概念—Pod Volume，后者有一些扩展，比如可以用分布式文件系统GlusterFS实现后端存储功能；Pod Volume是被定义在Pod上，然后被各个容器挂载到自己的文件系统中。

Event是一个事件的记录，记录了事件的最早产生时间、最后重现时间、重复次数、发起者、类型，以及导致此事件的原因等众多信息。Event通常会被关联到某个具体的资源对象上，是排查故障的重要参考信息。

在Kubernetes里，一个计算资源进行配额限定时需要设定以下两个参数：

- Requests：该资源的最小申请量，系统必须满足要求。
- Limits：该资源最大允许使用的量，不能被突破，当容器试图使用超过这个量的资源时，可能会被K8s杀掉并重启。

## Label

Label（标签）是一个key=value的键值对，其中key与value由用户自己指定。

Label可以被附加到各种资源对象上，例如Node、Pod、Service、RC等，一个资源对象可以定义任意数量的Label，同一个Label也可以被添加到任意数量的资源对象上。Label通常在资源对象定义时确定，也可以在对象创建后动态添加或者删除。

常见的Label有：版本标签release（stable、canary等）、环境标签environment、架构标签tier（frontend、backend、middleware等）、分区标签partition、质量管控标签track（daily、weekly）等等。

可以通过Label Selector（标签选择器）查询和筛选拥有某些Label的资源对象，Kubernetes通过这种方式实
现了类似SQL的简单又通用的对象查询机制。Selector可以使用等式（=, !=）、集合两种表达方式（in, notin）。matchLabels用于定义一组Label，与直接写在Selector中的作用相同；matchExpressions用于定义一组基于集合的筛选条件，可用的条件运算符包括In、NotIn、Exists和DoesNotExist。例如：

```
selector:
  matchLabels:
    component: redis
  matchExpressions:
    - {key: tier, operator: In, values: [cache]}
    - {key: environment, operator: NotIn, values: [dev]}
```

## Replication Controller（RC）

Replication Controller（简称RC），定义了一个期望的场景，即声明某种Pod的副本数量在任意时刻都符合某个预期值，所以RC的定义包括如下几个部分。

- Pod期待的副本数量。
- 用于筛选目标Pod的Label Selector。
- 当Pod的副本数量小于预期数量时，用于创建新Pod的Pod模板（template）

简单来说，RC的作用就是**确保Pod以你指定的副本数运行**。

定义了一个RC并将其提交到Kubernetes集群中后，Master上的Controller Manager组件就得到通知，定期巡检系统中当前存活的目标Pod，并确保目标Pod实例的数量刚好等于此RC的期望值，如果有过多的Pod副本在运行，系统就会停掉一些Pod，否则系统会再自动创建一些Pod。

例如：

```
apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx
spec:
  replicas: 3 （这里规定了要有3个对应的Pod在运行）
  selector:
    app: nginx （selector限定了哪些标签的Pod受到RC控制）
  template:
    metadata:
      name: nginx
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
```

删除RC并不会影响通过该RC已创建好的Pod。为了删除所有Pod，可以设置replicas的值为0，然后更新该RC。另外，kubectl提供了stop和delete命令来一次性删除RC和RC控制的全部Pod。

在Kubernetes 1.2中，RC升级为Replica Set，主要区别在于Replica Set可以通过集合语言限定Pod，而RC仅支持等式。

此外，当前很少单独使用Replica Set，它主要被Deployment这个更高层的资源对象所使用，从而形成一整套Pod创建、删除、更新的编排机制。我们在使用Deployment时，无须关心它是如何创建和维护Replica Set的，这一切都是自动发生的。

## Deployment

Deployment在内部使用了Replica Set来实现，用于更好地解决Pod的编排问题。

Deployment相对于RC的一大升级是可以随时知道当前Pod的部署进度。由于一个Pod的创建、调度、绑定节点及在目标Node上启动对应的容器这一完整过程需要一定的时间，所以系统启动N个Pod副本，实际上是一个连续变化的部署过程。

Deployment的**典型使用场景**有以下几个：

- 创建一个Deployment对象来生成对应的Replica Set并完成Pod副本的创建。
- 检查Deployment的状态来看部署动作是否完成（Pod副本数量是否达到预期的值）。
- 更新Deployment以创建新的Pod（比如镜像升级）。
- 如果当前Deployment不稳定，则回滚到一个早先的Deployment版本。
- 暂停Deployment以便于一次性修改多个PodTemplateSpec的配置项，之后再恢复Deployment，进行新的发布。
- 扩展Deployment以应对高负载。
- 查看Deployment的状态，以此作为发布是否成功的指标。
- 清理不再需要的旧版本ReplicaSets。

Deployment的定义与ReplicaSet的定义很类似：

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
```

Pod的管理对象，除了RC和Deployment，还包括ReplicaSet、DaemonSet、StatefulSet、Job等，分别用于不同的应用场景。

## Horizontal Pod Autoscaler（Pod横向自动扩容，HPA）

可以手动执行kubectl scale命令，进而实现Pod扩容或缩容。但是，根据Kubernetes的自动化、智能化的定位目标，分布式系统要能够根据当前负载的变化自动触发水平扩容或缩容，因为这一过程可能是频繁发生的、不可预料的，所以手动控制的方式是不现实的。

HPA与之前的RC、Deployment一样，也属于一种Kubernetes资源对象。通过追踪分析指定RC控制的所有目标Pod的负载变化情况，来确定是否需要有针对性地调整目标Pod的副本数量，这是HPA的实现原理。

当前，HPA有以下两种方式作为Pod负载的度量指标：

- CPUUtilizationPercentage，即目标Pod所有副本自身的CPU利用率的平均值。

  一个Pod自身的CPU利用率是该Pod当前CPU的使用量除以它的Pod Request的值；

  如果目标Pod没有定义Pod Request的值，则无法使用CPUUtilizationPercentage实现Pod横向自动扩容。

  在CPUUtilizationPercentage计算过程中使用到的Pod的CPU使用量通常是1min内的平均值。

  K8s自身孵化了一个基础性能数据采集监控框架——Kubernetes Monitoring Architecture。

- 应用程序自定义的度量指标，比如服务在每秒内的相应请求数（TPS或QPS）。

示例：

```
apiVersion: autoscaling/vl
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache
  namespace: default
spec:
  maxReplicas: 10 （Pod的副本数为1～10）
  minReplicas: 1
  scaleTargetRef:
    kind: Deployment （HPA控制的目标对象是一个名为php-apache的Deployment里的Pod副本）
    name: php-apache
  targetCPUUtilizationPercentage: 90 （超过90%时会触发自动动态扩容行为）
```

## StatefulSet

在Kubernetes系统中，Pod的管理对象RC、Deployment、DaemonSet和Job都面向无状态的服务。但现实中有很多服务是有状态的，特别是一些复杂的中间件集群，例如MySQL集群、MongoDB集群、ZooKeeper集群、Akka集群等，这些应用集群有4个共同点。

- 每个节点都有固定的身份ID，通过这个ID，集群中的成员可以相互发现并通信。
- 集群的规模是比较固定的，集群规模不能随意变动。
- 集群中的每个节点都是有状态的，通常会持久化数据到永久存储中。
- 如果磁盘损坏，则集群里的某个节点无法正常运行，集群功能受损。

如果通过RC或Deployment控制Pod副本数量来实现上述有状态的集群，就会发现第1点是无法满足的，因为Pod的名称是随机产生的，Pod的IP地址也是在运行期才确定且可能有变动的。

StatefulSet从本质上来说，可以看作Deployment/RC的一个特殊变种，它有如下特性：

- StatefulSet里的每个Pod都有稳定、唯一的网络标识，可以用来发现集群内的其他成员。假设StatefulSet的名称为kafka，那么第1个Pod叫kafka-0，第2个叫kafka-1，以此类推。
- StatefulSet控制的Pod副本的启停顺序是受控的，操作第n个Pod时，前n-1个Pod已经是运行且准备好的状态。
- StatefulSet里的Pod采用稳定的持久化存储卷，通过PV或PVC来实现，删除Pod时默认不会删除与StatefulSet相关的存储卷（为了保证数据的安全）

StatefulSet除了要与PV卷捆绑使用以存储Pod的状态数据，还要与Headless Service配合使用，即在每个StatefulSet定义中都要声明它属于哪个Headless Service。

## Service

Service其实就是微服务架构中的一个微服务，之前讲解Pod、RC等资源对象其实都是为讲解Kubernetes Service做铺垫的。

Service定义了一个服务的访问入口地址，前端的应用（Frontend Pod）通过这个入口地址访问其背后的一组由
Pod副本组成的集群实例，Service与其后端Pod副本集群之间则是通过Label Selector来实现无缝对接的。RC的作用实际上是保证Service的服务能力和服务质量始终符合预期标准。

Pod、RC与Service的逻辑关系：

```
(Frontend Pod) --> (Service) --> (Label Selector) ----> (Pod label:xxx)
                                         ↑        └---> (Pod label:xxx)
              (replcationController) ----┘        └---> (Pod label:xxx)
```

每个Pod都会被分配一个单独的IP地址，而且每个Pod都提供了一个独立的Endpoint（Pod IP+ContainerPort）以被客户端访问，并且多个Pod副本组成了一个集群来提供服务。

至于客户端如何来访问它们，一般的做法是部署一个**负载均衡器**（软件或硬件），为这组Pod开启一个对外的服务端口如8000端口，并且将这些Pod的Endpoint列表加入8000端口的转发列表，客户端就可以通过负载均衡器的对外IP地址以及服务端口来访问此服务。客户端的请求最后会被转发到哪个Pod，由负载均衡器的算法所决定。

运行在每个Node上的kube-proxy进程其实就是一个智能的软件负载均衡器，负责把对Service的请求转发到后端的某个Pod实例上，并在内部实现服务的负载均衡与会话保持机制。但Kubernetes发明了一种很巧妙又影响深远的设计：**Service没有共用一个负载均衡器的IP地址，每个Service都被分配了一个全局唯一的虚拟IP地址**，这个虚拟IP被称为`Cluster IP`。这样一来，每个服务就变成了具备唯一IP地址的通信节点，服务调用就变成了最基础的TCP网络通信问题。

Pod的Endpoint地址会随着Pod的销毁和重新创建而发生改变，因为新Pod的IP地址与之前旧Pod的不同。而Service一旦被创建，Kubernetes就会自动为它分配一个可用的Cluster IP，而且在Service的整个生命周期内，它的Cluster IP不会发生改变。

例：

```
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp （拥有app=MyApp标签的所有Pod都属于这个Service）
  ports:
    - protocol: TCP
      port: 80 （port属性则定义了Service的虚端口，此处服务端口为80）
      targetPort: 9376 （targetPort属性用来确定提供该服务的容器所暴露的端口号，未指定时与port相同）
```



## Job —— 批处理任务的资源对象

批处理任务通常并行（或者串行）启动多个计算进程去处理一批工作项（work item），在处理完成后，整个批处理任务结束。

可以通过Kubernetes Job这种新的资源对象，定义并启动一个批处理任务Job。

异同：与RC、Deployment、ReplicaSet、DaemonSet类似，Job也控制一组Pod容器。同时Job控制Pod副本与RC等控制器的工作机制有以下重要差别：

- Job所控制的Pod副本是短暂运行的，可以将其视为一组Docker容器，其中的每个Docker容器都仅仅运行一次。当Job控制的所有Pod副本都运行结束时，对应的Job也就结束了。Job在实现方式上与RC等副本控制器不同，Job生成的Pod副本是不能自动重启的，对应Pod副本的RestartPoliy都被设置为Never。因此，当对应的Pod副本都执行完成时，相应的Job也就完成了控制使命，即Job生成的Pod在Kubernetes中是短暂存在的。Kubernetes在1.5版本之后又提供了类似crontab的定时任务——CronJob，解决了某些批处理任务需要定时反复执行的问题。
- Job所控制的Pod副本的工作模式能够多实例并行计算，以TensorFlow框架为例，可以将一个机器学习的计算任务分布到10台机器上，在每台机器上都运行一个worker执行计算任务，这很适合通过Job生成10个Pod副本同时启动运算。







